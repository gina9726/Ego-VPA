
model:
    pretrain: checkpoints/pt/TSF-B/lavila_epo1.pth
    freeze_vis_backbone: true
    freeze_txt_backbone: true
    text_prompt:
        n_ctx: 8
        use_basis: true
    visual_prompt:
        num_layers: 12
        prompt_dim: 512
        num_tokens: 128 # 8x16=128
        #num_tokens: 64 # 8x8=64
        #num_tokens: 32 # 8x4=32
        deep: true
        deep_shared: false
        split_st: false
        pt_spt: true
        pt_tmp: false
        style: VoP_c_basis # prompts are generated by context fusion
        n_seg: 16     # number of segments per video (n_seg=clip_length -> 1 frame/seg)
        #n_seg: 8     # number of segments per video (n_seg=clip_length -> 1 frame/seg)
        #n_seg: 4     # number of segments per video (n_seg=clip_length -> 1 frame/seg)
        K_s: 8      # boundary of intra-frame/inter-frame attention (K_s=12: VoP_c, K_s=8: VoP_f+c)
        st: 0   # prompt tuning starting from st-th layer (default: first layer)
        end: 12 # prompt tuning starting from end-th layer (default: last layer)
        basis:
            size: 10

data:
    dataset: charades_ego
    root: /data/CharadesEgo/CharadesEgo_v1_480
    metadata: /data/CharadesEgo/CharadesEgo/metadata_filtered_train.pkl # all the training data
    metadata_val: /data/CharadesEgo/CharadesEgo/CharadesEgo_v1_test_only1st.csv
    clip_length: 16
    #clip_length: 4
    sparse_sample: true

training:
    lr: 0.01
    batch_size: 4
    epoch: 10
    use_checkpoint: true
    save_freq: 10
    eval_freq: 1
    lambda:  0.05  # coefficient for the prompt synthesis loss

exp: prompt/ego_vpa

